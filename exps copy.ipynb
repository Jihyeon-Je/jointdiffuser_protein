{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_collections\n",
    "import torch\n",
    "from torch import multiprocessing as mp\n",
    "from datasets import get_dataset\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import utils\n",
    "import einops\n",
    "#from torch.uatils._pytree import tree_map\n",
    "import accelerate\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from dpm_solver_pp import NoiseScheduleVP, DPM_Solver\n",
    "import tempfile\n",
    "from fid_score import calculate_fid_given_paths\n",
    "from absl import logging\n",
    "import builtins\n",
    "import os\n",
    "import wandb\n",
    "import libs.autoencoder\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils._pytree import tree_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_diffusion_beta_schedule(linear_start=0.00085, linear_end=0.0120, n_timestep=1000):\n",
    "    _betas = (\n",
    "        torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_timestep, dtype=torch.float64) ** 2\n",
    "    )\n",
    "    return _betas.numpy()\n",
    "\n",
    "\n",
    "def get_skip(alphas, betas):\n",
    "    N = len(betas) - 1\n",
    "    skip_alphas = np.ones([N + 1, N + 1], dtype=betas.dtype)\n",
    "    for s in range(N + 1):\n",
    "        skip_alphas[s, s + 1:] = alphas[s + 1:].cumprod()\n",
    "    skip_betas = np.zeros([N + 1, N + 1], dtype=betas.dtype)\n",
    "    for t in range(N + 1):\n",
    "        prod = betas[1: t + 1] * skip_alphas[1: t + 1, t]\n",
    "        skip_betas[:t, t] = (prod[::-1].cumsum())[::-1]\n",
    "    return skip_alphas, skip_betas\n",
    "\n",
    "\n",
    "def stp(s, ts: torch.Tensor):  # scalar tensor product\n",
    "    if isinstance(s, np.ndarray):\n",
    "        s = torch.from_numpy(s).type_as(ts)\n",
    "    extra_dims = (1,) * (ts.dim() - 1)\n",
    "    return s.view(-1, *extra_dims) * ts\n",
    "\n",
    "\n",
    "def mos(a, start_dim=1):  # mean of square\n",
    "    return a.pow(2).flatten(start_dim=start_dim).mean(dim=-1)\n",
    "\n",
    "\n",
    "def LSimple(x0, nnet, schedule, **kwargs):\n",
    "    n, eps, xn = schedule.sample(x0)  # n in {1, ..., 1000}\n",
    "    eps_pred = nnet(xn, n, **kwargs)\n",
    "    return mos(eps - eps_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_collections\n",
    "\n",
    "def d(**kwargs):\n",
    "    \"\"\"Helper of creating a config dict.\"\"\"\n",
    "    return ml_collections.ConfigDict(initial_dictionary=kwargs)\n",
    "    \n",
    "config = ml_collections.ConfigDict()\n",
    "config.seed = 1234\n",
    "config.z_shape = (4, 32, 32)\n",
    "config.config_name = 'test'\n",
    "config.ckpt_root = '/Users/jihyeonje/unidiffuser/test/'\n",
    "config.sample_dir = '/Users/jihyeonje/unidiffuser/test/'\n",
    "config.workdir = '/Users/jihyeonje/unidiffuser/test/'\n",
    "config.hparams = 'default'\n",
    "\n",
    "config.autoencoder = d(\n",
    "    pretrained_path='assets/stable-diffusion/autoencoder_kl.pth',\n",
    "    scale_factor=0.23010\n",
    ")\n",
    "config.train = d(\n",
    "    n_steps=5,\n",
    "    batch_size=2,\n",
    "    log_interval=10,\n",
    "    eval_interval=5,\n",
    "    save_interval=5\n",
    ")\n",
    "\n",
    "config.optimizer = d(\n",
    "    name='adamw',\n",
    "    lr=0.0002,\n",
    "    weight_decay=0.03,\n",
    "    betas=(0.9, 0.9)\n",
    ")\n",
    "\n",
    "config.lr_scheduler = d(\n",
    "    name='customized',\n",
    "    warmup_steps=5000\n",
    ")\n",
    "\n",
    "config.nnet = d(\n",
    "    name='uvit_t2i',\n",
    "    img_size=32,\n",
    "    in_chans=4,\n",
    "    patch_size=2,\n",
    "    embed_dim=512,\n",
    "    depth=12,\n",
    "    num_heads=8,\n",
    "    mlp_ratio=4,\n",
    "    qkv_bias=False,\n",
    "    mlp_time_embed=False,\n",
    "    clip_dim=768,\n",
    "    num_clip_token=77\n",
    ")\n",
    "\n",
    "config.dataset = d(\n",
    "    name='ligprot_features',\n",
    "    path='test/feats',\n",
    "    cfg=True,\n",
    "    p_uncond=0.1\n",
    ")\n",
    "\n",
    "config.sample = d(\n",
    "    sample_steps=2,\n",
    "    n_samples=3,\n",
    "    mini_batch_size=1,\n",
    "    cfg=True,\n",
    "    scale=1.,\n",
    "    path=''\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Schedule(object):  # discrete time\n",
    "    def __init__(self, _betas):\n",
    "        r\"\"\" _betas[0...999] = betas[1...1000]\n",
    "             for n>=1, betas[n] is the variance of q(xn|xn-1)\n",
    "             for n=0,  betas[0]=0\n",
    "        \"\"\"\n",
    "\n",
    "        self._betas = _betas\n",
    "        self.betas = np.append(0., _betas)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.N = len(_betas)\n",
    "\n",
    "        assert isinstance(self.betas, np.ndarray) and self.betas[0] == 0\n",
    "        assert isinstance(self.alphas, np.ndarray) and self.alphas[0] == 1\n",
    "        assert len(self.betas) == len(self.alphas)\n",
    "\n",
    "        # skip_alphas[s, t] = alphas[s + 1: t + 1].prod()\n",
    "        self.skip_alphas, self.skip_betas = get_skip(self.alphas, self.betas)\n",
    "        self.cum_alphas = self.skip_alphas[0]  # cum_alphas = alphas.cumprod()\n",
    "        self.cum_betas = self.skip_betas[0]\n",
    "        self.snr = self.cum_alphas / self.cum_betas\n",
    "\n",
    "    def tilde_beta(self, s, t):\n",
    "        return self.skip_betas[s, t] * self.cum_betas[s] / self.cum_betas[t]\n",
    "\n",
    "    def sample(self, x0):  # sample from q(xn|x0), where n is uniform\n",
    "        n = np.random.choice(list(range(1, self.N + 1)), (len(x0),))\n",
    "        eps = torch.randn_like(x0)\n",
    "        xn = stp(self.cum_alphas[n] ** 0.5, x0) + stp(self.cum_betas[n] ** 0.5, eps)\n",
    "        return torch.tensor(n, device=x0.device), eps, xn\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Schedule({self.betas[:10]}..., {self.N})'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'libs' has no attribute 'clip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/jihyeonje/unidiffuser/exps.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m prompts \u001b[39m=\u001b[39m [\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mA green train is coming down the tracks.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mA group of skiers are preparing to ski down a mountain.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mA close up of a plate of broccoli and sauce.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X31sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m clip \u001b[39m=\u001b[39m libs\u001b[39m.\u001b[39;49mclip\u001b[39m.\u001b[39mFrozenCLIPEmbedder()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m clip\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X31sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m clip\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'libs' has no attribute 'clip'"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    'A green train is coming down the tracks.',\n",
    "    'A group of skiers are preparing to ski down a mountain.',\n",
    "    'A small kitchen with a low ceiling.',\n",
    "    'A group of elephants walking in muddy water.',\n",
    "    'A living area with a television and a table.',\n",
    "    'A road with traffic lights, street lights and cars.',\n",
    "    'A bus driving in a city area with traffic signs.',\n",
    "    'A bus pulls over to the curb close to an intersection.',\n",
    "    'A group of people are walking and one is holding an umbrella.',\n",
    "    'A baseball player taking a swing at an incoming ball.',\n",
    "    'A city street line with brick buildings and trees.',\n",
    "    'A close up of a plate of broccoli and sauce.',\n",
    "]\n",
    "\n",
    "device = 'cpu'\n",
    "clip = libs.clip.FrozenCLIPEmbedder()\n",
    "clip.eval()\n",
    "clip.to(device)\n",
    "\n",
    "save_dir = f'test/feats/run_vis'\n",
    "latent = clip.encode(prompts)\n",
    "for i in range(len(latent)):\n",
    "    c = latent[i].detach().cpu().numpy()\n",
    "    np.save(os.path.join(save_dir, f'{i}.npy'), np.asarray((prompts[i], c), dtype='object'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs import clip\n",
    "prompts = [\n",
    "    '',\n",
    "]\n",
    "\n",
    "device = 'cpu'\n",
    "clip = clip.FrozenCLIPEmbedder()\n",
    "clip.eval()\n",
    "clip.to(device)\n",
    "\n",
    "save_dir = f'test/feats'\n",
    "latent = clip.encode(prompts)\n",
    "print(latent.shape)\n",
    "c = latent[0].detach().cpu().numpy()\n",
    "np.save(os.path.join(save_dir, f'empty_context.npy'), c)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = '/Users/jihyeonje/Downloads/PDBBind_processed/'\n",
    "ligpaths = []\n",
    "protpaths = []\n",
    "\n",
    "# iterate over files in\n",
    "# that directory\n",
    "for dir in os.listdir(directory):\n",
    "    if dir !='.DS_Store':\n",
    "        foldr = os.path.join(directory, dir)\n",
    "    for i in os.listdir(foldr):\n",
    "        if i.endswith('.sdf'):\n",
    "            ligpaths.append(os.path.join(foldr, i))\n",
    "        elif i.endswith('.pdb'):\n",
    "            protpaths.append(os.path.join(foldr, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import libs.autoencoder\n",
    "import libs.clip\n",
    "from datasets import LigProtDatabase\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "datas = LigProtDatabase(root='assets/datasets/coco/train2014',\n",
    "                             annFile='assets/datasets/coco/annotations/captions_train2014.json',\n",
    "                             size=32)\n",
    "save_dir = f'test/feats/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_dfs(protpaths, ligtxts):\n",
    "    max_id = len(protpaths)\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    valid_ids = np.random.choice(\n",
    "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    "    )\n",
    "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "    train_prot = list(map(protpaths.__getitem__, train_ids))\n",
    "    train_ligs = list(map(ligtxts.__getitem__, train_ids))\n",
    "    valid_prot = list(map(protpaths.__getitem__, valid_ids))\n",
    "    valid_ligs = list(map(ligtxts.__getitem__, valid_ids))\n",
    "    return train_prot, train_ligs, valid_prot, valid_ligs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prot, train_lig, valid_prot, valid_lig = make_train_valid_dfs(protpaths, ligpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = LigProtDatabase(protpaths = train_prot, ligpaths = train_lig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'LigProtDatabase' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jihyeonje/unidiffuser/exps.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#Y132sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mlen\u001b[39;49m(datas)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'LigProtDatabase' has no len()"
     ]
    }
   ],
   "source": [
    "len(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LigProtDatabase(Dataset):\n",
    "    def __init__(self, protpaths, ligpaths, size=None):\n",
    "        self.protpaths = protpaths\n",
    "        self.ligpaths = ligpaths\n",
    "        self.height = self.width = size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.protpaths)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        p_feats = load_feats_from_pdb(protpaths[index])\n",
    "        bb_coords = p_feats['bb_coords']\n",
    "        protein = einops.rearrange(p_feats, 'h w c -> c h w')\n",
    "        \n",
    "        suppl = Chem.SDMolSupplier(ligpaths[index], sanitize=False)\n",
    "        smi = Chem.MolToSmiles(suppl[0])\n",
    "        x, atom_types, one_hot = mol_extraction(smi)\n",
    "        ligand = torch.cat(x, torch.unsqueeze(one_hot,0), dim=2)\n",
    "        #image = np.ones((32,32,3))\n",
    "        \n",
    "        #key = self.keys[index]\n",
    "        #image = self._load_image(key)\n",
    "        #image = np.array(image).astype(np.uint8)\n",
    "        #image = center_crop(self.width, self.height, image).astype(np.float32)\n",
    "        \n",
    "        #image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "        #image = einops.rearrange(image, 'h w c -> c h w')\n",
    "\n",
    "        #anns = self._load_target(key)\n",
    "        #target = []\n",
    "        #for ann in anns:\n",
    "        #    target.append(ann['caption'])\n",
    "\n",
    "        return protein, ligand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    x = np.ones((4,32,32))*255\n",
    "    x = (x / 127.5 - 1.0).astype(np.float32)\n",
    "    #if len(x.shape) == 3:\n",
    "    #    x = x[None, ...]\n",
    "    #x = torch.tensor(x, device=device)\n",
    "    #moments = autoencoder(x, fn='encode_moments').squeeze(0)\n",
    "    #moments = moments.detach().cpu().numpy()\n",
    "    np.save(os.path.join(save_dir, f'{i}.npy'), x)\n",
    "\n",
    "    #latent = clip.encode(captions)\n",
    "    for j in range(3):\n",
    "        c = np.empty([77,768],dtype='f')\n",
    "        #c = latent[i].detach().cpu().numpy()\n",
    "        np.save(os.path.join(save_dir, f'{i}_{j}.npy'), c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prot, train_lig, valid_prot, valid_lig = make_train_valid_dfs(protpaths, ligpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LigProtFeatureDataset(Dataset):\n",
    "    # the image features are got through sample\n",
    "    def __init__(self, protpaths, ligpathss):\n",
    "        self.root = root\n",
    "        self.num_data, self.n_captions = len(protpaths), len(ligpaths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        z = np.load(os.path.join(self.root, f'{index}.npy'))\n",
    "        c = np.load(os.path.join(self.root, f'{index}_{k}.npy'))\n",
    "        return z, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LigProtFeatures(DatasetFactory):  # the moments calculated by Stable Diffusion image encoder & the contexts calculated by clip\n",
    "    def __init__(self, path, cfg=False, p_uncond=None):\n",
    "        super().__init__()\n",
    "        print('Prepare dataset...')\n",
    "        self.train = LigProtFeatureDataset(os.path.join(path, 'train'))\n",
    "        self.test = LigProtFeatureDataset(os.path.join(path, 'test'))\n",
    "        #assert len(self.train) == 82783\n",
    "        #assert len(self.test) == 40504\n",
    "        print('Prepare dataset ok')\n",
    "\n",
    "        self.empty_context = np.load(os.path.join(path, 'empty_context.npy'))\n",
    "\n",
    "        if cfg:  # classifier free guidance\n",
    "            assert p_uncond is not None\n",
    "            print(f'prepare the dataset for classifier free guidance with p_uncond={p_uncond}')\n",
    "            self.train = CFGDataset(self.train, p_uncond, self.empty_context)\n",
    "\n",
    "        # text embedding extracted by clip\n",
    "        # for visulization in t2i\n",
    "        self.prompts, self.contexts = [], []\n",
    "        for f in sorted(os.listdir(os.path.join(path, 'run_vis')), key=lambda x: int(x.split('.')[0])):\n",
    "            prompt, context = np.load(os.path.join(path, 'run_vis', f), allow_pickle=True)\n",
    "            self.prompts.append(prompt)\n",
    "            self.contexts.append(context)\n",
    "        self.contexts = np.array(self.contexts)\n",
    "\n",
    "    @property\n",
    "    def data_shape(self):\n",
    "        return 4, 32, 32\n",
    "\n",
    "    @property\n",
    "    def fid_stat(self):\n",
    "        return f'assets/fid_stats/fid_stats_mscoco256_val.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LigProtFeatureDataset(Dataset):\n",
    "    # the image features are got through sample\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.num_data, self.n_captions = get_feature_dir_info(root)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        z = np.load(os.path.join(self.root, f'{index}.npy'))\n",
    "        k = random.randint(0, self.n_captions[index] - 1)\n",
    "        c = np.load(os.path.join(self.root, f'{index}_{k}.npy'))\n",
    "        return z, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    x = np.ones((4,32,32))*255\n",
    "    x = (x / 127.5 - 1.0).astype(np.float32)\n",
    "    #if len(x.shape) == 3:\n",
    "    #    x = x[None, ...]\n",
    "    #x = torch.tensor(x, device=device)\n",
    "    #moments = autoencoder(x, fn='encode_moments').squeeze(0)\n",
    "    #moments = moments.detach().cpu().numpy()\n",
    "    np.save(os.path.join(save_dir, f'{i}.npy'), x)\n",
    "\n",
    "    #latent = clip.encode(captions)\n",
    "    for j in range(3):\n",
    "        c = np.empty([77,768],dtype='f')\n",
    "        #c = latent[i].detach().cpu().numpy()\n",
    "        np.save(os.path.join(save_dir, f'{i}_{j}.npy'), c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_loader.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(**config.dataset)\n",
    "\n",
    "test_dataset = dataset.get_split(split='test', labeled=True)  # for sampling\n",
    "test_dataset_loader = DataLoader(test_dataset, batch_size=config.sample.mini_batch_size, shuffle=True, drop_last=True,\n",
    "                                    num_workers=1, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(config):\n",
    "#    if config.get('benchmark', False):\n",
    "#        torch.backends.cudnn.benchmark = True\n",
    "#        torch.backends.cudnn.deterministic = False\n",
    "\n",
    "    #mp.set_start_method('spawn')\n",
    "    accelerator = accelerate.Accelerator()\n",
    "    device = accelerator.device\n",
    "    accelerate.utils.set_seed(config.seed, device_specific=True)\n",
    "    logging.info(f'Process {accelerator.process_index} using device: {device}')\n",
    "\n",
    "    config.mixed_precision = accelerator.mixed_precision\n",
    "    config = ml_collections.FrozenConfigDict(config)\n",
    "\n",
    "    assert config.train.batch_size % accelerator.num_processes == 0\n",
    "    mini_batch_size = config.train.batch_size // accelerator.num_processes\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        os.makedirs(config.ckpt_root, exist_ok=True)\n",
    "        os.makedirs(config.sample_dir, exist_ok=True)\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        wandb.init(dir=os.path.abspath(config.workdir), project=f'uvit_{config.dataset.name}', config=config.to_dict(),\n",
    "                   name=config.hparams, job_type='train', mode='offline')\n",
    "        utils.set_logger(log_level='info', fname=os.path.join(config.workdir, 'output.log'))\n",
    "        logging.info(config)\n",
    "    else:\n",
    "        utils.set_logger(log_level='error')\n",
    "        builtins.print = lambda *args: None\n",
    "    logging.info(f'Run on {accelerator.num_processes} devices')\n",
    "\n",
    "    dataset = get_dataset(**config.dataset)\n",
    "    #assert os.path.exists(dataset.fid_stat)\n",
    "    train_dataset = dataset.get_split(split='train', labeled=True)\n",
    "    train_dataset_loader = DataLoader(train_dataset, batch_size=mini_batch_size, shuffle=True, drop_last=True,\n",
    "                                      num_workers=1, pin_memory=True, persistent_workers=True)\n",
    "    test_dataset = dataset.get_split(split='test', labeled=True)  # for sampling\n",
    "    test_dataset_loader = DataLoader(test_dataset, batch_size=config.sample.mini_batch_size, shuffle=True, drop_last=True,\n",
    "                                     num_workers=1, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "    train_state = utils.initialize_train_state(config, device)\n",
    "    nnet, nnet_ema, optimizer, train_dataset_loader, test_dataset_loader = accelerator.prepare(\n",
    "        train_state.nnet, train_state.nnet_ema, train_state.optimizer, train_dataset_loader, test_dataset_loader)\n",
    "    lr_scheduler = train_state.lr_scheduler\n",
    "    train_state.resume(config.ckpt_root)\n",
    "\n",
    "    #autoencoder = libs.autoencoder.get_model(**config.autoencoder)\n",
    "    #autoencoder.to(device)\n",
    "\n",
    "    #@ torch.cuda.amp.autocast()\n",
    "    #def encode(_batch):\n",
    "    #    return autoencoder.encode(_batch)\n",
    "\n",
    "    #@ torch.cuda.amp.autocast()\n",
    "    #def decode(_batch):\n",
    "    #    return autoencoder.decode(_batch)\n",
    "\n",
    "    def get_data_generator():\n",
    "        while True:\n",
    "            for data in tqdm(train_dataset_loader, disable=not accelerator.is_main_process, desc='epoch'):\n",
    "                yield data\n",
    "\n",
    "    data_generator = get_data_generator()\n",
    "\n",
    "    def get_context_generator():\n",
    "        while True:\n",
    "            for data in test_dataset_loader:\n",
    "                _, _context = data\n",
    "                yield _context\n",
    "\n",
    "    context_generator = get_context_generator()\n",
    "\n",
    "    _betas = stable_diffusion_beta_schedule()\n",
    "    _schedule = Schedule(_betas)\n",
    "    logging.info(f'use {_schedule}')\n",
    "\n",
    "    def cfg_nnet(x, timesteps, context):\n",
    "        _cond = nnet_ema(x, timesteps, context=context)\n",
    "        _empty_context = torch.tensor(dataset.empty_context, device=device)\n",
    "        _empty_context = einops.repeat(_empty_context, 'L D -> B L D', B=x.size(0))\n",
    "        _uncond = nnet_ema(x, timesteps, context=_empty_context)\n",
    "        return _cond + config.sample.scale * (_cond - _uncond)\n",
    "\n",
    "    def train_step(_batch):\n",
    "        _metrics = dict()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        _z = _batch[0]\n",
    "        print(_z.shape)\n",
    "        loss = LSimple(_z, nnet, _schedule, context=_batch[1])  # currently only support the extracted feature version\n",
    "        _metrics['loss'] = accelerator.gather(loss.detach()).mean()\n",
    "        accelerator.backward(loss.mean())\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        train_state.ema_update(config.get('ema_rate', 0.9999))\n",
    "        train_state.step += 1\n",
    "        return dict(lr=train_state.optimizer.param_groups[0]['lr'], **_metrics)\n",
    "\n",
    "    def dpm_solver_sample(_n_samples, _sample_steps, **kwargs):\n",
    "        _z_init = torch.randn(_n_samples, *config.z_shape, device=device)\n",
    "        noise_schedule = NoiseScheduleVP(schedule='discrete', betas=torch.tensor(_betas, device=device).float())\n",
    "\n",
    "        def model_fn(x, t_continuous):\n",
    "            t = t_continuous * _schedule.N\n",
    "            return cfg_nnet(x, t, **kwargs)\n",
    "\n",
    "        dpm_solver = DPM_Solver(model_fn, noise_schedule, predict_x0=True, thresholding=False)\n",
    "        _z = dpm_solver.sample(_z_init, steps=_sample_steps, eps=1. / _schedule.N, T=1.)\n",
    "        return _z\n",
    "\n",
    "    def eval_step(n_samples, sample_steps):\n",
    "        logging.info(f'eval_step: n_samples={n_samples}, sample_steps={sample_steps}, algorithm=dpm_solver, '\n",
    "                     f'mini_batch_size={config.sample.mini_batch_size}')\n",
    "\n",
    "        def sample_fn(_n_samples):\n",
    "            _context = next(context_generator)\n",
    "            assert _context.size(0) == _n_samples\n",
    "            return dpm_solver_sample(_n_samples, sample_steps, context=_context)\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as temp_path:\n",
    "            path = config.sample.path or temp_path\n",
    "            if accelerator.is_main_process:\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "            utils.sample2dir(accelerator, path, n_samples, config.sample.mini_batch_size, sample_fn, dataset.unpreprocess)\n",
    "\n",
    "            _fid = 0\n",
    "            if accelerator.is_main_process:\n",
    "                _fid = calculate_fid_given_paths((dataset.fid_stat, path))\n",
    "                logging.info(f'step={train_state.step} fid{n_samples}={_fid}')\n",
    "                with open(os.path.join(config.workdir, 'eval.log'), 'a') as f:\n",
    "                    print(f'step={train_state.step} fid{n_samples}={_fid}', file=f)\n",
    "                wandb.log({f'fid{n_samples}': _fid}, step=train_state.step)\n",
    "            _fid = torch.tensor(_fid, device=device)\n",
    "            _fid = accelerator.reduce(_fid, reduction='sum')\n",
    "\n",
    "        return _fid.item()\n",
    "\n",
    "    logging.info(f'Start fitting, step={train_state.step}, mixed_precision={config.mixed_precision}')\n",
    "\n",
    "    step_fid = []\n",
    "    while train_state.step < config.train.n_steps:\n",
    "        nnet.train()\n",
    "        batch = tree_map(lambda x: x.to(device), next(data_generator))\n",
    "        metrics = train_step(batch)\n",
    "\n",
    "        nnet.eval()\n",
    "        if accelerator.is_main_process and train_state.step % config.train.log_interval == 0:\n",
    "            logging.info(utils.dct2str(dict(step=train_state.step, **metrics)))\n",
    "            logging.info(config.workdir)\n",
    "            wandb.log(metrics, step=train_state.step)\n",
    "\n",
    "        if accelerator.is_main_process and train_state.step % config.train.eval_interval == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            logging.info('Save a grid of images...')\n",
    "            #contexts = torch.tensor(dataset.contexts, device=device)[: 2 * 5]\n",
    "            #samples = dpm_solver_sample(_n_samples=2 * 5, _sample_steps=50, context=contexts)\n",
    "            #samples = make_grid(dataset.unpreprocess(samples), 5)\n",
    "            #save_image(samples, os.path.join(config.sample_dir, f'{train_state.step}.png'))\n",
    "            #wandb.log({'samples': wandb.Image(samples)}, step=train_state.step)\n",
    "            torch.cuda.empty_cache()\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "        if train_state.step % config.train.save_interval == 0 or train_state.step == config.train.n_steps:\n",
    "            torch.cuda.empty_cache()\n",
    "            logging.info(f'Save and eval checkpoint {train_state.step}...')\n",
    "            if accelerator.local_process_index == 0:\n",
    "                train_state.save(os.path.join(config.ckpt_root, f'{train_state.step}.ckpt'))\n",
    "            accelerator.wait_for_everyone()\n",
    "            fid = eval_step(n_samples=1, sample_steps=1)  # calculate fid of the saved checkpoint\n",
    "            step_fid.append((train_state.step, fid))\n",
    "            torch.cuda.empty_cache()\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "    logging.info(f'Finish fitting, step={train_state.step}')\n",
    "    logging.info(f'step_fid: {step_fid}')\n",
    "    step_best = sorted(step_fid, key=lambda x: x[1])\n",
    "    print(step_best)\n",
    "    logging.info(f'step_best: {step_best}')\n",
    "    ##note\n",
    "    train_state.load(os.path.join(config.ckpt_root, f'5.ckpt'))\n",
    "    accelerator.wait_for_everyone()\n",
    "    #eval_step(n_samples=10000, sample_steps=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:autoencoder:\n",
      "  pretrained_path: assets/stable-diffusion/autoencoder_kl.pth\n",
      "  scale_factor: 0.2301\n",
      "ckpt_root: /Users/jihyeonje/unidiffuser/test/\n",
      "config_name: test\n",
      "dataset:\n",
      "  cfg: true\n",
      "  name: ligprot_features\n",
      "  p_uncond: 0.1\n",
      "  path: test/feats\n",
      "hparams: default\n",
      "lr_scheduler:\n",
      "  name: customized\n",
      "  warmup_steps: 5000\n",
      "mixed_precision: 'no'\n",
      "nnet:\n",
      "  clip_dim: 768\n",
      "  depth: 12\n",
      "  embed_dim: 512\n",
      "  img_size: 32\n",
      "  in_chans: 4\n",
      "  mlp_ratio: 4\n",
      "  mlp_time_embed: false\n",
      "  name: uvit_t2i\n",
      "  num_clip_token: 77\n",
      "  num_heads: 8\n",
      "  patch_size: 2\n",
      "  qkv_bias: false\n",
      "optimizer:\n",
      "  betas: !!python/tuple\n",
      "  - 0.9\n",
      "  - 0.9\n",
      "  lr: 0.0002\n",
      "  name: adamw\n",
      "  weight_decay: 0.03\n",
      "sample:\n",
      "  cfg: true\n",
      "  mini_batch_size: 1\n",
      "  n_samples: 3\n",
      "  path: ''\n",
      "  sample_steps: 2\n",
      "  scale: 1.0\n",
      "sample_dir: /Users/jihyeonje/unidiffuser/test/\n",
      "seed: 1234\n",
      "train:\n",
      "  batch_size: 2\n",
      "  eval_interval: 5\n",
      "  log_interval: 10\n",
      "  n_steps: 5\n",
      "  save_interval: 5\n",
      "workdir: /Users/jihyeonje/unidiffuser/test/\n",
      "z_shape: !!python/tuple\n",
      "- 4\n",
      "- 32\n",
      "- 32\n",
      "\n",
      "INFO:absl:Run on 1 devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare dataset...\n",
      "Prepare dataset ok\n",
      "prepare the dataset for classifier free guidance with p_uncond=0.1\n",
      "attention mode is math\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:nnet has 44692644 parameters\n",
      "/var/folders/4h/2m4bntjn2wgdjw3zcvz8l96w0000gn/T/ipykernel_56567/1977945471.py:21: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.snr = self.cum_alphas / self.cum_betas\n",
      "INFO:absl:use Schedule([0.         0.00085    0.0008547  0.00085941 0.00086413 0.00086887\n",
      " 0.00087362 0.00087839 0.00088316 0.00088795]..., 1000)\n",
      "INFO:absl:Start fitting, step=0, mixed_precision=no\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4894d8bb1a5d4a9ea4730683b3812d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 32, 32])\n",
      "orig x\n",
      "torch.Size([2, 4, 32, 32])\n",
      "patch\n",
      "torch.Size([2, 256, 512])\n",
      "context\n",
      "torch.Size([2, 77, 768])\n",
      "torch.Size([2, 77, 512])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jihyeonje/unidiffuser/exps.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(config)\n",
      "\u001b[1;32m/Users/jihyeonje/unidiffuser/exps.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X12sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m nnet\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X12sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m batch \u001b[39m=\u001b[39m tree_map(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mto(device), \u001b[39mnext\u001b[39m(data_generator))\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X12sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m metrics \u001b[39m=\u001b[39m train_step(batch)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X12sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m nnet\u001b[39m.\u001b[39meval()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X12sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m \u001b[39mif\u001b[39;00m accelerator\u001b[39m.\u001b[39mis_main_process \u001b[39mand\u001b[39;00m train_state\u001b[39m.\u001b[39mstep \u001b[39m%\u001b[39m config\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mlog_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/Users/jihyeonje/unidiffuser/exps.ipynb Cell 14\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X12sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m _metrics[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m accelerator\u001b[39m.\u001b[39mgather(loss\u001b[39m.\u001b[39mdetach())\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X12sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m accelerator\u001b[39m.\u001b[39mbackward(loss\u001b[39m.\u001b[39mmean())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X12sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X12sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jihyeonje/unidiffuser/exps.ipynb#X12sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m train_state\u001b[39m.\u001b[39mema_update(config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mema_rate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0.9999\u001b[39m))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/unidiffuser/lib/python3.9/site-packages/accelerate/optimizer.py:140\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_overflow \u001b[39m=\u001b[39m scale_after \u001b[39m<\u001b[39m scale_before\n\u001b[1;32m    139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep(closure)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/unidiffuser/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/unidiffuser/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/unidiffuser/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/unidiffuser/lib/python3.9/site-packages/torch/optim/adamw.py:147\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    144\u001b[0m state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m,), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat, device\u001b[39m=\u001b[39mp\u001b[39m.\u001b[39mdevice) \\\n\u001b[1;32m    145\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mcapturable\u001b[39m\u001b[39m'\u001b[39m] \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.\u001b[39m)\n\u001b[1;32m    146\u001b[0m \u001b[39m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m state[\u001b[39m'\u001b[39m\u001b[39mexp_avg\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros_like(p, memory_format\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mpreserve_format)\n\u001b[1;32m    148\u001b[0m \u001b[39m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m    149\u001b[0m state[\u001b[39m'\u001b[39m\u001b[39mexp_avg_sq\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(p, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coords_from_pdb(\n",
    "    pdb,\n",
    "    atoms,\n",
    "    method=\"raw\",\n",
    "    also_bfactors=False,\n",
    "    normalize_bfactors=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns array of shape (1, n_res, len(atoms), 3)\n",
    "    \"\"\"\n",
    "\n",
    "    coords = []\n",
    "    bfactors = []\n",
    "    if method == \"raw\":  # Raw numpy implementation, faster than biopdb\n",
    "        # Indexing into PDB format, allowing XXXX.XXX\n",
    "        coords_in_pdb = [slice(30, 38), slice(38, 46), slice(46, 54)]\n",
    "        # Indexing into PDB format, allowing XXX.XX\n",
    "        bfactor_in_pdb = slice(60, 66)\n",
    "\n",
    "        with open(pdb, \"r\") as f:\n",
    "            resi_prev = 1\n",
    "            counter = 0\n",
    "            for l in f:\n",
    "                l_split = l.rstrip(\"\\n\").split()\n",
    "                if len(l_split) > 0 and l_split[0] == \"ATOM\" and l_split[2] in atoms:\n",
    "                    resi = l_split[5]\n",
    "                    if resi == resi_prev:\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        counter = 0\n",
    "                    if counter < len(atoms):\n",
    "                        xyz = [\n",
    "                            np.array(l[s].strip()).astype(float) for s in coords_in_pdb\n",
    "                        ]\n",
    "                        coords.append(xyz)\n",
    "                        if also_bfactors:\n",
    "                            bfactor = np.array(l[bfactor_in_pdb].strip()).astype(float)\n",
    "                            bfactors.append(bfactor)\n",
    "                    resi_prev = resi\n",
    "            coords = torch.Tensor(np.array(coords)).view(1, -1, len(atoms), 3)\n",
    "\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import protein\n",
    "import residue_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feats_from_pdb(pdb, bb_atoms=[\"N\", \"CA\", \"C\", \"O\"]):\n",
    "    \"\"\"\n",
    "    Load model input features from a PDB file or mmcif file.\n",
    "    - bb_atoms: list of backbone atom names to load\n",
    "    - load_atom73: if True, also load atom73 features\n",
    "    - chain_residx_gap: residue index gap for chain breaks for PDBs with multiple chains\n",
    "    \"\"\"\n",
    "    feats = {}\n",
    "    protein_obj = protein.read_pdb(pdb)\n",
    "    bb_idxs = [residue_constants.atom_order[a] for a in bb_atoms]\n",
    "    bb_coords = torch.from_numpy(protein_obj.atom_positions[:, bb_idxs])\n",
    "    feats[\"bb_coords\"] = bb_coords.float()\n",
    "    for k, v in vars(protein_obj).items():\n",
    "        feats[k] = torch.Tensor(v)\n",
    "    feats[\"aatype\"] = feats[\"aatype\"].long()\n",
    "    #if load_atom73:\n",
    "    #    feats[\"atom73_coords\"], feats[\"atom73_mask\"] = atom37_to_atom73(\n",
    "    #        feats[\"atom_positions\"], feats[\"aatype\"], return_mask=True\n",
    "    #    )\n",
    "\n",
    "    # Handle residue index for PDBs with multiple chains    \n",
    "    #feats[\"residue_index\"] = add_chain_gap(feats[\"residue_index\"], feats[\"chain_index\"], chain_residx_gap=chain_residx_gap)\n",
    "\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = load_feats_from_pdb(protpaths[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([830, 37, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats['atom_positions'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "830"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feats['aatype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([830, 4, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats['bb_coords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import Chem\n",
    "def mol_extraction(smi):\n",
    "    atom_encoder = {'H': 0, 'C': 1, 'N': 2, 'O': 3, 'F': 4}\n",
    "    atom_decoder = ['H', 'C', 'N', 'O', 'F'] \n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "\n",
    "    #start processing 3D coordinates\n",
    "    updated_mol= Chem.AddHs(mol)\n",
    "    AllChem.EmbedMolecule(updated_mol)\n",
    "    AllChem.UFFOptimizeMolecule(updated_mol)\n",
    "    updated_mol.GetConformer()\n",
    "    #to get atom type, coordinate, edges\n",
    "    mblock = Chem.MolToMolBlock(updated_mol)\n",
    "    parsed = mblock.split(\"\\n\")\n",
    "    stats = parsed[3]\n",
    "    tot_atoms = int(stats.split(\" \")[1])\n",
    "    one_hot = torch.zeros(tot_atoms, len(atom_decoder))\n",
    "\n",
    "    coors = parsed[4:tot_atoms+4]\n",
    "    allcoors = []\n",
    "    allatoms = []\n",
    "    \n",
    "    for j in range(tot_atoms):\n",
    "        atom_type = coors[j][:-37][-1]\n",
    "        xyz_coor = [float(x) for x in coors[j][:-37][:-2].split()]\n",
    "        allcoors.append(xyz_coor)\n",
    "        atom = atom_encoder[atom_type]\n",
    "        allatoms.append(atom)\n",
    "        one_hot[j, atom_encoder[atom_type]] = 1\n",
    "        \n",
    "    return allcoors, allatoms, one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mean(x):\n",
    "    mean = torch.mean(x, dim=1, keepdim=True)\n",
    "    x = x - mean\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "smi = \"CC1=NCCC(C)O1\"\n",
    "#smi = \"[H]C(=O)N([H])C([H])(C([H])([H])[H])C([H])([H])OC([H])([H])C([H])([H])[H]\"\n",
    "id2charge = {'H': 1, 'C': 6, 'N': 7, 'O': 8, 'F': 9}\n",
    "new_data = {}\n",
    "x, atom_types, one_hot = mol_extraction(smi)\n",
    "#at_types = torch.from_numpy(np.asarray(atom_types))[:, None]\n",
    "# one_hot = torch.eq(at_types, torch.Tensor(atomic_number_list)).int()\n",
    "#one_hot = F.one_hot(\n",
    "#                torch.from_numpy(np.asarray(atom_types)),\n",
    "##                num_classes=5)\n",
    "#one_hot = one_hot[None,:]\n",
    "#x = remove_mean(torch.tensor([x]))\n",
    "\n",
    "n = x.shape[1]\n",
    "\n",
    "new_data['positions'] = x\n",
    "new_data['one_hot'] = torch.unsqueeze(one_hot, 0)\n",
    "\n",
    "c = torch.zeros(1,len(atom_types),1)\n",
    "h = {'categorical': torch.unsqueeze(one_hot, 0), 'integer': c}\n",
    "\n",
    "new_data['atom_mask'] = torch.ones((1,n), device='cpu').unsqueeze(2)\n",
    "\n",
    "edge_mask = torch.ones((n, n), device='cpu')\n",
    "edge_mask[~torch.eye(edge_mask.shape[0], dtype=torch.bool)] = 0\n",
    "new_data['edge_mask'] = edge_mask.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xh = torch.cat([new_data['positions'], h['categorical'], h['integer']], dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 19, 9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = nn.Linear(9,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unidiffuser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
